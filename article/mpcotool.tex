\documentclass[review,authoryear]{elsarticle}

\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{pstricks}
\usepackage{multido}
\usepackage{amsfonts}
\usepackage[hyphens]{url}
\expandafter\def\expandafter\UrlBreaks\expandafter{\UrlBreaks
  \do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j
  \do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t
  \do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D
  \do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N
  \do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X
  \do\Y\do\Z}
\usepackage{multirow}
\usepackage{tabu}
\usepackage{lineno}
\linenumbers

\newcommand{\EQ}[2]
{\begin{equation}#1\label{#2}\end{equation}}

\newcommand{\PICTURE}[5]
{
	\begin{figure}[ht!]
		\centering
		\begin{picture}(#1,#2)
			#3
		\end{picture}
		\caption{#4.\label{#5}}
	\end{figure}
}

\newcommand{\PSPICTURE}[7]
{
	\begin{figure}[ht!]
		\centering
		\pspicture(#1,#2)(#3,#4)
			#5
		\endpspicture
		\caption{#6.\label{#7}}
	\end{figure}
}

\newcommand{\TABLE}[5]
{
	\begin{table}[ht!]
		\centering
		\caption{#4.\label{#5}}
		#1
		\tabulinesep=0.9mm
		\begin{tabu}{#2}
			#3
		\end{tabu}
	\end{table}
}

\newcommand{\FIG}[3]
{
	\begin{figure}[ht!]
		\centering
		\includegraphics[width=\textwidth]{#1}
		\caption{#2.\label{#3}}
	\end{figure}
}

\newcommand{\PLOT}[3]
{
	\begin{figure}[ht!]
		\centering
		\includegraphics{#1}
		\caption{#2.\label{#3}}
	\end{figure}
}

\newcommand{\FIGII}[4]
{
	\begin{figure}[ht!]
		\centering
		\begin{tabular}{c}
			\includegraphics{#1} \\ \includegraphics{#2}
		\end{tabular}
		\caption{#3.\label{#4}}
	\end{figure}
}

\newcommand{\PLOTII}[4]
{
	\begin{figure}[ht!]
		\centering
		\begin{tabular}{cc}
			\includegraphics{#1} & \includegraphics{#2}
		\end{tabular}
		\caption{#3.\label{#4}}
	\end{figure}
}

\newcommand{\FIGIII}[5]
{
	\begin{figure}[ht!]
		\centering
		\begin{tabular}{cc}
			\includegraphics{#1} & \includegraphics{#2} \\
			\multicolumn{2}{c}{\includegraphics{#3}}
		\end{tabular}
		\caption{#4.\label{#5}}
	\end{figure}
}

\newcommand{\FIGIV}[6]
{
	\begin{figure}[ht!]
		\centering
		\begin{tabular}{cc}
			\includegraphics{#1} & \includegraphics{#2} \\
			\includegraphics{#3} & \includegraphics{#4}
		\end{tabular}
		\caption{#5.\label{#6}}
	\end{figure}
}

\newcommand{\FIGVI}[8]
{
	\begin{figure}[ht!]
		\centering
		\begin{tabular}{cc}
			\includegraphics{#1} & \includegraphics{#2} \\
			\includegraphics{#3} & \includegraphics{#4} \\
			\includegraphics{#5} & \includegraphics{#6}
		\end{tabular}
		\caption{#7.\label{#8}}
	\end{figure}
}

\newcommand{\ABS}[1]{\left|#1\right|}
\newcommand{\C}[1]{\left[#1\right]}
\newcommand{\MATRIX}[2]{\PA{\begin{array}{#1}#2\end{array}}}
\newcommand{\PA}[1]{\left(#1\right)}
\newcommand{\LL}[1]{\left\{#1\right\}}

\bibliographystyle{elsarticle-harv}

\begin{document}

\title{MPCOTool: an open source software to supply empirical parameters
required in simulation models. I: model and tests}

\author[eead,bifi]{J. Burguete\corref{cor1}}
\ead{jburguete@eead.csic.es}

\author[unizar]{A. Lacasta}
\ead{alacasta@unizar.es}

\author[eead]{B. Latorre}
\ead{borja.latorre@csic.es}

\author[kit]{S. Ambroj}
\ead{samuel.ambroj@kit.edu}

\author[eead]{S. Ouazaa}
\ead{sofiane.ouazaa@eead.csic.es}

\author[eead]{N. Zapata}
\ead{v.zapata@csic.es}

\author[unizar]{P. García-Navarro}
\ead{pigar@unizar.es}

\cortext[cor1]{Corresponding author}

\address[eead]{Soil and Water, EEAD / CSIC.
P.O. Box 13034, 50080~Zaragoza, Spain.}
\address[bifi]{BIFI: Instituto de Biocomputación y Física de Sistemas Complejos,
Universidad de Zaragoza.
Mariano Esquillor, Edificio I+D, 50009~Zaragoza, Spain.}
\address[kit]{Steinbuch Centre for Computing (SCC),
Karlsruhe Institute of Technology (KIT).
KIT-Campus Nord, Hermann von Helmholtzplatz 1, 76344 Eggenstein - Leopoldshafen,
Germany.}
\address[unizar]{Fluid Mechanics, LIFTEC, CSIC-Universidad de Zaragoza.
María de Luna 3, 50018~Zaragoza, Spain.}

\begin{keyword}
optimization, calibration, simulation, model, software, irrigation, sprinkler,
furrow, canal
\end{keyword}

\begin{abstract}
The present work describes MPCOTool, the Multi-Purposes Calibration or
Optimization Tool, a new software program to perform
optimization or calibration of empirical parameters required in the
formulation of numerical simulation models. The structure is such that it can be
easily adapted to different external simulation codes.
One genetic algorithm, two brute force optimization algorithms (sweep and
Monte-Carlo), one iterative algorithm
and one direction search algorithm are included as alternative methods.
Parallel computations are enabled in a simple way so that the work load can be
distributed among the different processors available in one computer or in
multiple computers of a cluster.
The software optimization methods are analyzed in six standard analytic test
functions.
This software is open source and is distributed with a BSD (Berkeley Software
Distribution) type license.
In a next companion paper the MPCOTool usage and possibilities are illustrated
by showing four practical applications in agriculture.
\end{abstract}

\maketitle

\section{Introduction}

Calibration is related to the optimization process of finding the minimum of a
function usually defined as a relation between the desired value of a quantity
and that supplied by a suitable simulation model. This process is more necessary
when simplifications on models are done as some of the
physical processes are hidden in black box parameters. Deterministic models are
sensitive to parameters and their ability increases when the quality of the
calibration improves. In some way, the model may acquire a true predictive
character when calibrated. The calibration can be understood as an optimization
process where the error between the estimation of the model and the data set is
minimized \citep{Lacasta16}.

There are many fields where calibration is required. Traditionally, fields such
as hydrology or weather forecasting have used mathematical models with empirical
parameters which require to be adjusted. For example, watershed models usually
estimate initial parameter values for sediment fractions, instream water
temperature or infiltration rates \citep{Duan04}. Within agriculture, there are
many phenomena that can be modelled using differential equations
\citep{Playan06,JaviSurcos2,Ebrahimiam13,Ouazaa14,Ouazaa15,SedagatdoostEbraimian15}.
The application of
mathematical models to describe physical processes is helpful to perform
simulations of possible scenarios using computational facilities. Despite the
quality of these mathematical models for predictive purposes, calibration is
usually required to provide them with useful properties. The
shallow-water equations for the simulation of open-channel flow, for instance,
have been widely used, in particular for irrigation water delivery analysis. To
complete the equations with regulation elements, it is accepted that some
simplifications can be applied. These simplifications usually include parameters
to model those regulation elements.

Because of the mathematical model, differential equations constrained
optimization may be performed using two optimizer families: gradient based
methods and gradient-free optimizers. While the latter do not require gradient
information to perform the optimization, the former must have information about
the variation of the objective with respect to the controlled variable
\citep{Lacasta16}. Therefore, although gradient based methods exhibit superior
convergences, they are more difficult to implement in general purpose software.
Nonetheless, when calibration is understood as the adjustment of empirical
values, gradient-based methods may fail due to the location of local minima.
Stochastic based optimization algorithms (as sweep, Monte-Carlo or genetic) are
not affected by local minimum presence. 
Detailed analysis of the properties and performance of several stochastic
optimization algorithms can be seen, for instance, in \citet{Back96} or in
\citet{HauptHaupt04}.
Applications of these methods in
agriculture can be seen for instance in \citet{JaviSurcos2} (sweep),
\citet{Ouazaa15} (Monte-Carlo) or \citet{Ebrahimiam13} (genetic). Moreover,
these methods are easily parallelizable increasing the performance when executed
on machines with multiple processors. In \cite{Lacasta16} a deeper comparison of
both, gradient based and stochastic optimization methods can be found.

In this work MPCOTool, the Multi-Purposes Calibration and Optimization Tool
\citep{MPCOToolGit}, a software to perform optimization or calibration of
empirical parameters required in simulation models, is presented. It implements
one genetic algorithm, two brute force algorithms (sweep and Monte-Carlo) and,
to improve the brute force methods, one iterative algorithm and one direction
search algorithm. The software is open source with a BSD type license. All input
data files used in the manuscript are also free and they can be downloaded in
the cited web page.

The present paper is organized as follows: First, the concepts of calibration
and optimization are presented as the main goal. Then, the methods used in MPCOTool are described. The tool structure and organization
are next presented in detail. After that,
the software optimization methods are analyzed in six standard analytic test
functions. Finally, the conclusions are drawn. 
Subsequently, in a next companion paper the performance is demonstrated using
four different applications of interest in agriculture.

\section{Calibration and optimization}

Calibration can be considered a particular case of optimization
\citep{WrightNocedal99}, probably the most widely applied. It usually deals with
setting the parameters that allow to represent correctly the behavior of a
component or model. It is very common to describe physical phenomena using
complex mathematical models including all the possible scales. Nevertheless, it
is very useful to simplify them including adjustable parameters.

Optimization can be understood as the minimization process of some functional
that evaluates the objective of a model. Therefore, a functional $J$ must be
defined in order to evaluate the deviation between the model results
$\mathbf{x}$ and the objectives $\mathbf{x}_o$, with
$\mathbf{x},\mathbf{x}_o$ real vectors of $n$
components ($x_i$ and $x_{o,i}$ respectively). 
Infinite ways of defining $J$ can be used, for example penalizing different
variables with different weights according to their importance.
The aim is to find the minimum of this functional.

In order to meet the optimal solution it is important to keep in mind that
$\mathbf{x}\equiv\mathbf{x}\PA{\mathbf{y}}$ may be directly or indirectly
related with another tunable quantity: $\mathbf{y}$  ($m$
parameters which can be optimized in the model). The complexity of the
optimization process comes from the relation that may be established between the
model and this tunable set of parameters. Bearing in mind that relation, the
optimization is the minimization of the functional by means of the modification
of those parameters. Moreover, those parameters are usually defined in a range
$y_i\in\PA{y_{i,min},\,y_{i,max}}$ so the minimization problem is formally
to find the optimum set of parameters $\mathbf{y}_o$ that:
\EQ
{
	J\PA{\mathbf{x}\PA{\mathbf{y}_o}}
	=\min_{\mathbf{y}}J\PA{\mathbf{x}\PA{\mathbf{y}}}.
}{EqParametersOptimal}
MPCOTool allows to find them by means of different methods. In addition,
the metric used to evaluate the functional is open and may be defined by the
user.

\section{Optimization methods}

The optimization methods implemented in MPCOTool are next presented.
Details of the algorithm and parallelization characteristics, as well as
flowcharts, examples and diagrams of all methods described in this section can
be seen in the user manual provided with the software.

\subsection{Sweep brute force method (SW)}

The sweep brute force method finds the optimal set of parameters within a solution region by dividing it into regular sub-domains. To find the optimal solution, the domain interval is first defined for each variable $x_i \in \PA{x_{i,min},\,x_{i,max}}$. Then, a regular partition in  $N_{x,i}$ sub-intervals is made. This method is the most obvious and simplest algorithm of optimization. Moreover, it is so old that it is difficult to find references of first applications. Note that the computational cost increases exponentially as the number of variables to optimize grows.

Brute force algorithms present low convergence rates but they are strongly
parallelizable because every simulation is completely independent. If the
computer, or the computers cluster, can execute $N_{tasks}$ parallel tasks
every task does $N_{total}/N_{tasks}$ simulations, obviously taking into account
rounding effects (every task has to perform a natural number of simulations).

\subsection{Monte-Carlo method (MC)}

Monte-Carlo based methods run $N_s$ simulations using aleatory values of the
variables assuming uniform probability within the extreme values range. This is
another old brute force method attributed to different authors. In
\citet{AtanassovDimov08} an interesting analysis of this method is presented.
The algorithm is parallelizable in the same way as the sweep algorithm.

\subsection{Iterative algorithm (IT) applied to brute force methods}

MPCOTool allows to iterate both SW or MC brute force methods in
order to seek convergence. In this case, the best results from the previous
iteration are used to force new intervals in the variables for the following
iteration. Then, for $N_b^j$, the subset of the best simulation results in
the $j$-th iteration, the following quantities are defined:
\begin{description}
\item[$\displaystyle x_{\max}^b=\max_{i\in N_b}x_i^j$]: Maximum value of the
	variable $x$ in the subset of the best simulation results from the $j$-th
	iteration.
\item[$\displaystyle x_{\min}^b=\max_{i\in N_b}x_i^j$]: Minimum value of the
	variable $x$ in the subset of the best simulation results from the $j$-th
	iteration.
\end{description}
A new interval in the variable $x$ is defined to build the optimization values in the next $(j+1)$ iteration so that:
\EQ{x_i^{j+1}\in\left[x_{\min}^{j+1},\;x_{\max}^{j+1}\right],}
{EqIterationInterval}
with:
\[
	\mathrm{SW}\;\Rightarrow\;\left\{\begin{array}{c}
	\displaystyle
	x_{\max}^{j+1}=x_{\max}^b+\frac{x_{\max}^j-x_{\min}^j}{N_x-1}\,tol,\\
	\displaystyle
	x_{\min}^{j+1}=x_{\min}^b-\frac{x_{\min}^j-x_{\min}^j}{N_x-1}\,tol,\\
	\end{array}\right.
\]
\EQ
{
	\mathrm{MC}\;\Rightarrow\;\left\{\begin{array}{c}
	\displaystyle x_{\max}^{j+1}=\frac{x_{\max}^b+x_{\min}^b
	+\left(x_{\max}^b-x_{\min}^b\right)(1+tol)}{2},\\
	\displaystyle x_{\min}^{j+1}=\frac{x_{\max}^b+x_{\min}^b
	-\left(x_{\max}^b-x_{\min}^b\right)(1+tol)}{2},
	\end{array}\right.
}{EqIterationTolerance}
being $tol$ a tolerance factor increasing the size of the variable intervals to
simulate the next iteration. Note that this factor affects in different manner
to SW and MC algorithms. The method is repeated $N_i$ times.

The iterative algorithm can be also easily parallelized. However, this method is
less parallelizable than pure brute force methods because the parallelization
has to be performed for each iteration.

\subsection{Direction search method (DS)}

Brute force optimization methods, SW and MC, can be also combined
with a direction search algorithm. Defining the vector $\vec{r}_i$ as the optimum
variables combination obtained in the $i$-th step, $\vec{r}_1$ as the optimum
variables combination vector obtained by the brute force method and $\vec{s}_i$ as:
\EQ
{
	\vec{s}_1=\vec{0},\qquad
	\vec{s}_i=(1-rel)\,\vec{s}_{i-1}+rel\,\Delta\vec{r}_{i-1},
}{Eqs}
with $\Delta\vec{r}_{i-1}=\vec{r}_i+\vec{r}_{i-1}$ and $rel$ the
relaxation parameter, the DS method checks $N_e$
variable combinations and selects the optimum as:
\EQ
{
	\vec{r}_{i+1}
	=\mathrm{optimum}\PA{\vec{r}_i,\;\vec{r}_i+\vec{s}_i+\vec{t}_j},
	\;j=1,\cdots,N_e.
}{EqDirection}
If the step does not improve the optimum ($\vec{r}_i=\vec{r}_{i+1}$) then the
direction step vectors $\vec{t}_j$ are divided by two and $\vec{s}_{i+1}$ is set
to zero. The method is iterated $N_{st}$ times.

Although direction search method gets the fastest convergence, it is the method
in MPCOTool that obtains the least advantages from parallelization. The method is
almost sequential and parallelization can be only performed for each step in the
$N_e$ simulations to estimate the direction.

MPCOTool uses two methods to build the $\vec{t}_j$ vectors:

\subsubsection{Coordinates descent (CD)}

This method builds the $\vec{t}_j$ vectors by increasing or decreasing only one
variable:
\EQ
{
	\vec{t}_1=\MATRIX{c}{st_1\\0\\\vdots\\0},\quad
	\vec{t}_2=\MATRIX{c}{-st_1\\0\\\vdots\\0},\quad
	\cdots\quad,\vec{t}_{N_e}=\MATRIX{c}{0\\0\\\vdots\\-st_{N_v}},
}{EqtDescent}
being $N_v$ the variables number and $st_j$ the initial step size for the $j$-th
variable defined by the user in the main input file. The number of estimates in
this method depends on $N_v$:
\EQ{N_e=2\,N_v}{EqNestimatesDescent}

\subsubsection{Random (RA)}

The vectors $\vec{t}_j$ are built randomly as:
\EQ
{
	\vec{t}_j=\MATRIX{c}{\PA{1-2\,r_{j,1}}\,st_1\\\vdots\\
	\PA{1-2\,r_{j,N_v}}\,st_{N_v}},
}{EqtRandom}
with $r_{j,k}\in[0,1)$ random numbers.

\subsection{Genetic method (GE)}

Genetic method bases have already been established in \citet{Holland75}.
MPCOTool also offers the use of a genetic method Genetic \citep{genetic} with
its default algorithms. It is inspired on the ideas in \citet{gaul}, but it has
been fully reprogrammed involving more modern external libraries. The code in
Genetic is also open source under a BSD license.

\subsubsection{The genome}

The variables to calibrate/optimize are coded in Genetic using a bit chain: the
genome. The larger the number of bits assigned to a variable the higher the resolution.
The number of bits assigned to each variable, and therefore the genome size, is fixed and the same for all the 
simulations.
The value assigned to a variable $x$ is determined by the extreme values $x_{\min}$ and $x_{\max}$, the binary number assigned in the genome to variable $I_x$ and by the number of bits assigned to variable $N_x$ according to
the following formula:
\EQ{x=x_{\min}+\frac{I_x}{2^{N_x}}\,\left(x_{\max}-x_{\min}\right).}{EqGenome}

\subsubsection{Survival of the best individuals}

In a population with $N_p$ individuals, in the first generation all the cases are simulated. The input variables are
taken from the genome of each individual. Next, in every generation, $N_p\,R_m$ individuals are generated by mutation, $N_p\,R_r$ individuals are generated by reproduction and $N_p\,R_a$ individuals are generated by adaptation, obviously
taking into account rounding. On second and further generations only simulations
associated to this new individuals ($N_{new}$) have to be run:
\EQ{N_{new}=N_p\,\left(R_m+R_r+R_a\right).}{EqNew}
Then, the total number of simulations performed by the genetic algorithm is:
\EQ{N_{total}=N_p+\left(N_g-1\right)\,N_{new},}{EqGeneticNumber}
with $N_g$ the number of generations of new entities.
The individuals of the former population that obtained lower values in the evaluation function are replaced so that the best $N_{survival}$ individuals survive:
\EQ{N_{survival}=N_p-N_{new}.}{EqSurvival}
Furthermore, the ancestors to generate new individuals are chosen among the surviving population. Obviously, to have survival population, the following condition has to be enforced:
\EQ{R_m+R_r+R_a<1}{EqSurvivalCondition}
MPCOTool uses a default aleatory criterion in Genetic, with a probability linearly decreasing with the ordinal in the ordered set of surviving individuals.

\subsubsection{Mutation algorithm}

In the mutation algorithm an identical copy of the parent genome is made except for a bit, randomly chosen with uniform probability, which is inverted.

\subsubsection{Reproduction algorithm}

The default algorithm in Genetic selects two different parents with one of the least errors after the 
complete simulation of one generation. 
A new individual is then generated by sharing the common bits of both parents and a random choice in the others.
The new child has the same number of bits as the parents but different genome.

\subsubsection{Adaptation algorithm}

Another algorithm is included in Genetic called "adaptation" although, in the
biological sense, it would rather be a smooth mutation. First, one of the
variables codified in the genome is randomly selected with uniform probability.
Then, a bit is randomly chosen assuming a probability linearly decreasing with
the significance of the bit. The new individual receives a copy of the parent
genome with the selected bit inverted.

This algorithm is rather similar to the mutation algorithm previously described but, since the probability to affect less significant bits is larger, so is the probability to produce smaller changes.

\subsubsection{Parallelization}

This method is also easily parallelizable following a similar scheme to the
iterative algorithm although, in the same way, pure brute force methods allow
better parallelization because genetic algorithm requires one for each
generation.

\section{Implementation}

\subsection{Organization of MPCOTool}

Let us assume that $N_{par}$ empirical parameters are desired so
that the results from a simulation model are the best fit to $N_{exp}$
experimental data and that the simulator requires $N_{in}$ input files. The
structure followed by MPCOTool is summarized in the \emph{main input file},
where both $N_{exp}$ and $N_{in}$ are specified. Furthermore, it
contains the extreme values of the empirical parameters and the chosen
optimization algorithm. Then, MPCOTool reads the
$N_{exp}\,N_{in}$ templates to build the simulator input files
replacing key labels by empirical parameter values created by the optimization
algorithm. There are two options: either the simulator compares directly the
simulation results with the \emph{experimental data file}, hence generating a
file with the value of the error, or an external program called \emph{evaluator}
is invoked to compare with the \emph{experimental data file} and to produce the
error value. In both cases this error value is saved in an
\emph{objective value file}. Then for each experiment, an objective value $o_i$
is obtained. The final value of the objective function ($J$) associated with the
experiments set can be calculated by four different error norms:
\EQ{L_2:\quad J=\sqrt{\sum_{i=1}^{N_{exp}}\ABS{w_i\,o_i}^2},}
{EqObjectiveFunctionLII}
\EQ{L_\infty:\quad J=\max_{i=1}^{N_{exp}}\ABS{w_i\,o_i},}
{EqObjectiveFunctionLi}
\EQ{L_p:\quad J=\sqrt[p]{\sum_{i=1}^{N_{exp}}\ABS{w_i\,o_i}^p},}
{EqObjectiveFunctionLp}
\EQ{L_1:\quad J=\sum_{i=1}^{N_{exp}}\ABS{w_i\,o_i},}
{EqObjectiveFunctionLI}
with $w_i$ the weight associated to the $i$-th experiment and $p$ the exponent
of the $L_p$ norm, specified both in the \emph{main input file}.
Note that all considered error norms are equivalent for only one experiment to
optimize ($J=\ABS{w_1\,o_1}$).
Note also that these error norms are used to combine the objective value $o_i$
for each experiment, but the user is completely free to define this function.
Fig.~\ref{FigStructure} is a sketch of the MPCOTool structure.
\psset{xunit=0.4mm,yunit=0.4mm}
\PSPICTURE{-20}{-115}{260}{55}
{
	\tiny
	\rput(10,50){Main input file}
	\psframe(-20,45)(40,55)
	\psline{->}(40,50)(50,50)
	\rput(10,25){1st template file}
	\psframe(-20,20)(40,30)
	\psline{->}(40,25)(50,25)
	\psline[linestyle=dotted,dotsep=1pt]{->}(50,25)(90,25)
	\rput(10,15){$\cdots$}
	\rput(10,5){$n$-th template file}
	\psframe(-20,0)(40,10)
	\psline{->}(40,5)(50,5)
	\psline[linestyle=dotted,dotsep=1pt]{->}(50,5)(90,5)
	\rput(10,-35){$\cdots$}
	\rput(10,-75){$(N\,n)$-th template file}
	\psframe(-20,-70)(40,-80)
	\psline{->}(40,-75)(50,-75)
	\psline[linestyle=dotted,dotsep=1pt]{->}(50,-75)(90,-75)
	\rput(70,50){MPCOTool}
	\psframe(50,-95)(90,55)
	\rput(70,-110){Objective function value}
	\psframe(35,-105)(105,-115)
	\psline{->}(70,-95)(70,-105)
	\psline{->}(90,25)(100,25)
	\psline{->}(90,5)(100,5)
	\psline{->}(90,-55)(100,-55)
	\psline{->}(90,-75)(100,-75)
	\rput(120,5){$n$-th input file}
	\psframe(100,0)(140,10)
	\psline{->}(140,5)(150,5)
	\rput(120,15){$\cdots$}
	\rput(120,25){1st input file}
	\psframe(100,20)(140,30)
	\psline{->}(140,25)(145,25)(145,5)
	\rput(175,5){Simulator}
	\psframe(150,0)(200,10)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(175,10)(175,20)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(200,5)(210,7.5)
	\rput(175,25){Results file}
	\psframe[linestyle=dashed,dash=3pt 1pt](150,20)(200,30)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(200,25)(210,30)
	\rput(175,45){Experimental}
	\rput(175,40){data file}
	\psframe(150,35)(200,50)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(200,42.5)(210,30)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(150,42.5)(145,42.5)(145,25)
	\rput(230,30){Evaluator}
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(230,25)(230,15)
	\psframe[linestyle=dashed,dash=3pt 1pt](210,25)(250,35)
	\rput(230,10){Objective}
	\rput(230,5){value file}
	\psframe(210,0)(250,15)
	\psline{->}(250,7.5)(260,7.5)(260,-90)(90,-90)
	\psline[linestyle=dotted,dotsep=1pt]{->}(90,-90)(70,-90)(70,-95)
	\rput(120,50){1st experiment}
	\psframe[linestyle=dotted](95,-5)(255,55)
	\rput(175,-15){$\cdots$}
	\rput(120,-75){$n$-th input file}
	\psframe(100,-80)(140,-70)
	\psline{->}(140,-75)(150,-75)
	\rput(120,-65){$\cdots$}
	\rput(120,-55){1st input file}
	\psframe(100,-60)(140,-50)
	\psline{->}(140,-55)(145,-55)(145,-75)
	\rput(175,-75){Simulator}
	\psframe(150,-80)(200,-70)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(175,-70)(175,-60)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(200,-75)(210,-72.5)
	\rput(175,-55){Results file}
	\psframe[linestyle=dashed,dash=3pt 1pt](150,-60)(200,-50)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(200,-55)(210,-50)
	\rput(175,-35){Experimental}
	\rput(175,-40){data file}
	\psframe(150,-30)(200,-45)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(200,-37.5)(210,-50)
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(150,-37.5)(145,-37.5)(145,-55)
	\rput(230,-50){Evaluator}
	\psline[linestyle=dashed,dash=2pt 1pt]{->}(230,-55)(230,-65)
	\psframe[linestyle=dashed,dash=3pt 1pt](210,-55)(250,-45)
	\rput(230,-70){Objective}
	\rput(230,-75){value file}
	\psframe(210,-80)(250,-65)
	\psline(250,-72.5)(260,-72.5)
	\rput(120,-30){$N$-th experiment}
	\psframe[linestyle=dotted](95,-85)(255,-25)
}{Flowchart of the interactions among MPCOTool, the input files and the
simulation and evaluation programs to produce an objective function value for
each empirical parameters combination generated by the optimization algorithm}
{FigStructure}

The whole process is repeated for each combination of empirical parameters generated by the optimization algorithm. Furthermore, MPCOTool automatically parallelizes the simulations using all the available computing resources.

The required format for the main input file and the template files are described in the provided user manual.

\subsection{Command line format}

The code of MPCOTool is written in C using standard open source libraries. The software can be compiled in the most widely used operative systems (Windows, Linux, FreeBSD, ...). Instructions to build the executable can be read in the user manual. The optional arguments for the command line will be typed in square brackets.

\begin{itemize}

\item Command line in sequential mode (where X is the number of threads to
execute and S is a seed for the pseudo-random numbers generator):
\begin{lstlisting}[language=bash,basicstyle=\scriptsize]
> ./mpcotoolbin [-nthreads X] [-seed S] input_file.xml
[result_file] [variables_file]
\end{lstlisting}

\item Command line in parallelized mode (where in this case X is the number of
threads to open for every node):
\begin{lstlisting}[language=bash,basicstyle=\scriptsize]
> mpirun [MPI options] ./mpcotoolbin [-nthreads X] [-seed S]
input_file.xml [result_file] [variables_file]
\end{lstlisting}

\item The syntax of the simulator program has to be:
\begin{lstlisting}[language=bash,basicstyle=\scriptsize]
> ./simulator_name input_file_1 [input_file_2] [...] output_file
\end{lstlisting}
There are two options for the output file. It can begin with a number indicating
the objective function value or it can be a results file that has to be
evaluated by an external program (the evaluator) comparing with an experimental
data file.

\item In the last option of the former point, the syntax of the program to
evaluate the objective function has to be (where the results file has to begin
with the objective function value):
\begin{lstlisting}[language=bash,basicstyle=\scriptsize]
> ./evaluator_name simulated_file experimental_file results_file
\end{lstlisting}

\end{itemize}

\subsection{Graphical user interface application}

A more user friendly application with an interactive graphical user interface is
named as \emph{MPCOTool}. This application displays a window, as the one
represented in Fig.~\ref{FigWindow}, to select all algorithms,
parameters, data and executable files required in the simulation.
\FIG{mpcotool-en.eps}{Main window of the interactive graphical user interface
application}{FigWindow}

\section{Optimization analytic tests}

Several common tests are presented below (as can be seen in Wikipedia
\url{https://en.wikipedia.org/wiki/Test_functions_for_optimization})
in order to test the performance of the different optimization 
algorithms. In all cases, the optimization of two parameters which minimize
an analytic function is attempted, having each problem an 
analytic solution as well. The tests were obtained from \citet{SurjanovicBingam15},
where they are classified as follows:
\begin{itemize}
\item Bowl-shaped function: Sphere function.
\item Many local minima function: Ackley's function.
\item Plate-shaped function: Booth's function.
\item Valley-shaped function: Rosenbrock's function.
\item Step ridges/drops function: Easom's function.
\item Other type function: Beale's function.
\end{itemize}
Table~\ref{TabOriginalTests} displays the absolute minimum and the domain used
for the optimization of the aforementioned functions.
\TABLE{\footnotesize}{ccc}
{
	Objective function & Minimum & Domain \\
	\hline
	\multirow{2}{*}{$f_{Sphere}(x,\,y)=x^2+y^2$} &
	\multirow{2}{*}{$f_{Sphere}(0,\,0)=0$} &
	$x\in[-5,5]$ \\ & & $y\in[-5,5]$ \\
	$f_{Ackley}(x,\,y)=20\,\C{1-\exp\PA{-\frac15\,\sqrt{\frac{x^2+y^2}{2}}}}$
	& \multirow{2}{*}{$f_{Ackley}(0,\,0)=0$} &
	$x\in[-40,40]$ \\
	$+e-\exp\C{\frac{\cos(2\,\pi\,x)+\cos(2\,\pi\,y)}{2}}$ &
	& $y\in[-40,40]$ \\
	\multirow{2}{*}{$f_{Booth}(x,\,y)=(x+2\,y-7)^2+(2\,x+y-5)^2$} &
	\multirow{2}{*}{$f_{Booth}(1,\,3)=0$} &
	$x\in[-10,10]$ \\ & & $y\in[-10,10]$ \\
	\multirow{2}{*}{$f_{Rosenbrock}(x,\,y)=100\,\PA{y-x^2}^2+(x-1)^2$} &
	\multirow{2}{*}{$f_{Rosenbrock}(1,\,1)=0$} &
	$x\in[-5,10]$ \\ & & $y\in[-5,10]$ \\
	$f_{Easom}(x,\,y)=-\cos(x)\,\cos(y)$ &
	\multirow{2}{*}{$f_{Easom}(\pi,\,\pi)=-1$} &
	$x\in[-100,100]$ \\
	$\exp\LL{-\C{(x-\pi)^2+(y-\pi)^2}}$ &
	& $y\in[-100,100]$ \\
	$f_{Beale}(x,\,y)=(1.5-x+x\,y)^2$ &
	\multirow{2}{*}{$f_{Beale}\PA{3,\,\frac12}=0$} &
	$x\in[-5,5]$ \\
	$+\PA{2.25-x+x\,y^2}^2+\PA{2.625-x+x\,y^3}^2$ &
	& $y\in[-5,5]$
}{Some standard functions to check the performance of the optimization
algorithms}{TabOriginalTests}

The algorithms based on the sweep method, as the space between the variables
is traversed at regular intervals, are likely to exactly fall in the minimum
of the function when the parameters to be minimized are integer or rational
numbers. In those cases, the function has been modified in order to get 
the minimum with irrational parameters. Moreover, negative functions will be
translated because the error norms have to be defined positive. Modified
functions are presented in Table~\ref{TabModifiedTests}.
\TABLE{\footnotesize}{ccc}
{
	Objective function & Minimum & Domain \\
	\hline
	\multirow{2}{*}{$g_{Sphere}(x,\,y)
		=f_{Sphere}\PA{x-\frac{\pi}{4},\,y-\frac{\pi}{4}}$} &
		\multirow{2}{*}{$g_{Sphere}\PA{\frac{\pi}{4},\,\frac{\pi}{4}}=0$} &
	$x\in[-5,5]$ \\ & & $y\in[-5,5]$ \\
	\multirow{2}{*}{$g_{Ackley}(x,\,y)
		=f_{Ackley}\PA{x-\frac{\pi}{4},\,y-\frac{\pi}{4}}$} &
	\multirow{2}{*}{$g_{Ackley}\PA{\frac{\pi}{4},\,\frac{\pi}{4}}=0$} &
	$x\in[-40,40]$ \\ & & $y\in[-40,40]$ \\
	\multirow{2}{*}{$g_{Booth}(x,\,y)
		=f_{Booth}\PA{x-\frac{\pi}{4},\,y-\frac{\pi}{4}}$} &
	\multirow{2}{*}{$g_{Booth}\PA{1+\frac{\pi}{4},\,3+\frac{\pi}{4}}=0$} &
	$x\in[-10,10]$ \\ & & $y\in[-10,10]$ \\
	$g_{Rosenbrock}(x,\,y)$ &
	$g_{Rosenbrock}\PA{1+\frac{\pi}{4},\,1+\frac{\pi}{4}}$ &
	$x\in[-5,10]$ \\ 
	$=f_{Rosenbrock}\PA{x-\frac{\pi}{4},\,y-\frac{\pi}{4}}$ &
	$=0$ &
	$y\in[-5,10]$ \\
	\multirow{2}{*}{$g_{Easom}(x,\,y)=1+f_{Easom}(x,y)$} &
	\multirow{2}{*}{$g_{Easom},(\pi,\pi)=0$} &
	$x\in[-100,100]$ \\ & & $y\in[-100,100]$ \\
	\multirow{2}{*}{$g_{Beale}(x,\,y)
		=f_{Beale}\PA{x-\frac{\pi}{4},\,y-\frac{\pi}{4}}$} &
	\multirow{2}{*}{$g_{Beale}\PA{3+\frac{\pi}{4},\,\frac12+\frac{\pi}{4}}=0$} &
	$x\in[-5,5]$ \\ & & $y\in[-5,5]$
}{Modified standard functions to analyze the optimization algorithms performance
in MPCOTool}{TabModifiedTests}

When presenting the results, the following notation is used:
\begin{description}
\item[$N_{simulated}$]: number of performed simulations. 
\item[$\tilde{x}_i$]: value of the parameter $x$ for the best of the $i$
	performed simulations.
\item[$\overline{x}$]: optimum value of the parameter $x$.
\item[$d_i=\sqrt{\PA{\tilde{x}_i-\overline{x}}^2
	+\PA{\tilde{y}_i-\overline{y}_i}^2}$]: distance between the best of the $i$
	performed simulations and the optimum value of the parameters (in the case
    of an optimization problem with two variables).
\end{description}

The value of the modified functions in the $x-y$ plane is shown in
Fig.~\ref{FigTests}.
\FIGVI{Sphere.eps}{Ackley.eps}{Booth.eps}{Rosenbrock.eps}{Easom.eps}{Beale.eps}
{Map in the $x-y$ plane showing the values of the objective functions used as 
test}{FigTests}
Fig.~\ref{FigTestsZoom} shows a zoomed area of some of these functions to 
highlight their peculiarities.
\PLOTII{Ackley2.eps}{Easom2.eps}{A zoom of (left) Ackley and (right) Easom
objective functions}{FigTestsZoom}

The ability of the different algorithms to obtain the optimum
value for all the tests, using a fixed number of total simulations of 2500 in each method,
is then presented. The abbreviations shown in table~\ref{TabMethods}, related 
to the different combinations of algorithms and configuration parameters, are used
for the sake of simplicity.
\TABLE{\scriptsize}{ccc}
{
	Abbreviations & Algorithms & Parameters \\
	\hline
	SW-1 & SW & $N_x=N_y=50$ \\
	SW+IT-1 & SW+IT & $N_x=N_y=10$, $N_i=25$, $N_b=10$, $tol=0.5$ \\
	SW+IT-2 & SW+IT & $N_x=N_y=10$, $N_i=25$, $N_b=4$, $tol=0$ \\
	SW+CD-1 & SW+DS+CD & $N_x=N_y=10$, $N_{st}=600$, $st_x=st_y=0.1$, $rel=1$ \\
	SW+CD-2 & SW+DS+CD & $N_x=N_y=40$, $N_{st}=225$, $st_x=st_y=0.01$,
		$rel=1$ \\
	SW+RA-1 & SW+DS+RA & $N_x=N_y=10$, $N_{st}=1200$, $st_x=st_y=0.1$, $rel=1$,
		$N_e=2$ \\
	SW+RA-2 & SW+DS+RA & $N_x=N_y=10$, $N_{st}=600$, $st_x=st_y=0.1$, $rel=1$,
		$N_e=4$ \\
	SW+RA-3 & SW+DS+RA & $N_x=N_y=10$, $N_{st}=240$, $st_x=st_y=0.1$, $rel=1$,
		$N_e=10$ \\
	SW+RA-4 & SW+DS+RA & $N_x=N_y=40$, $N_{st}=450$, $st_x=st_y=0.01$,
		$rel=1$, $N_e=2$ \\
	SW+RA-5 & SW+DS+RA & $N_x=N_y=40$, $N_{st}=225$, $st_x=st_y=0.01$,
		$rel=1$, $N_e=4$ \\
	SW+RA-6 & SW+DS+RA & $N_x=N_y=40$, $N_{st}=90$, $st_x=st_y=0.01$,
		$rel=1$, $N_e=10$ \\
	MC-1 & MC & $N_s=2500$ \\
	MC+IT-1 & MC+IT & $N_s=100$, $N_i=25$, $N_b=10$, $tol=0.1$ \\
	MC+IT-2 & MC+IT & $N_s=100$, $N_i=25$, $N_b=4$, $tol=0$ \\
	MC+CD-1 & MC+DS+CD & $N_s=100$, $N_{st}=600$, $st_x=st_y=0.1$, $rel=1$ \\
	MC+CD-2 & MC+DS+CD & $N_s=1600$, $N_{st}=225$, $st_x=st_y=0.01$, $rel=1$ \\
	MC+CD-3 & MC+DS+CD & $N_s=100$, $N_{st}=600$, $st_x=st_y=0.01$, $rel=0$ \\
	MC+CD-4 & MC+DS+CD & $N_s=100$, $N_{st}=600$, $st_x=st_y=0.01$, $rel=1$ \\
	MC+CD-5 & MC+DS+CD & $N_s=100$, $N_{st}=600$, $st_x=st_y=0.01$, $rel=2$ \\
	MC+CD-6 & MC+DS+CD & $N_s=100$, $N_{st}=600$, $st_x=st_y=0.1$, $rel=0$ \\
	MC+CD-7 & MC+DS+CD & $N_s=100$, $N_{st}=600$, $st_x=st_y=1$, $rel=0$ \\
	MC+RA-1 & MC+DS+RA & $N_s=100$, $N_{st}=1200$, $st_x=st_y=0.1$, $rel=1$,
		$N_e=2$ \\
	MC+RA-2 & MC+DS+RA & $N_s=100$, $N_{st}=600$, $st_x=st_y=0.1$, $rel=1$,
		$N_e=4$ \\
	MC+RA-3 & MC+DS+RA & $N_s=100$, $N_{st}=240$, $st_x=st_y=0.1$, $rel=1$,
		$N_e=10$ \\
	MC+RA-4 & MC+DS+RA & $N_x=1600$, $N_{st}=450$, $st_x=st_y=0.01$, $rel=1$,
		$N_e=2$ \\
	MC+RA-5 & MC+DS+RA & $N_x=1600$, $N_{st}=225$, $st_x=st_y=0.01$, $rel=1$,
		$N_e=4$ \\
	MC+RA-6 & MC+DS+RA & $N_x=1600$, $N_{st}=90$, $st_x=st_y=0.01$, $rel=1$,
		$N_e=10$ \\
	GE-1 & GE & $N_p=100$, $N_g=33$, $R_m=R_r=0$, $R_a=0.75$, $N_{bits}=32$ \\
	GE-2 & GE & $N_p=100$, $N_g=33$, $R_m=R_a=0$, $R_r=0.75$, $N_{bits}=32$ \\
	GE-3 & GE & $N_p=100$, $N_g=33$, $R_m=R_r=R_a=0.25$, $N_{bits}=32$ \\
	GE-4 & GE & $N_p=100$, $N_g=33$, $R_m=0.75$, $R_r=R_a=0$, $N_{bits}=32$ \\
	GE-5 & GE & $N_p=250$, $N_g=16$, $R_m=R_r=R_a=0.2$, $N_{bits}=32$ \\
	GE-6 & GE & $N_p=250$, $N_g=31$, $R_m=R_r=R_a=0.1$, $N_{bits}=32$ \\
	GE-7 & GE & $N_p=500$, $N_g=21$, $R_m=R_r=0$, $R_a=0.2$, $N_{bits}=32$ \\
	GE-8 & GE & $N_p=500$, $N_g=21$, $R_m=R_a=0$, $R_r=0.2$, $N_{bits}=32$ \\
	GE-9 & GE & $N_p=500$, $N_g=21$, $R_m=0.2$, $R_r=R_a=0$, $N_{bits}=32$ \\
	GE-10 & GE & $N_p=625$, $N_g=11$, $R_m=R_r=R_a=0.1$, $N_{bits}=32$ \\
	GE-11 & GE & $N_p=625$, $N_g=6$, $R_m=R_r=R_a=0.2$, $N_{bits}=32$
}{Abbreviations of the methods used in the results of the tests}
{TabMethods}

Next, the behavior of the different optimization algorithms for the sphere function
is analyzed, being a symmetric, convex, without local extremes and easy to optimize
function.

Fig.~\ref{FigSphereVariables} displays all the possible combinations of the
simulated variables by the different algorithms in order to illustrate how
each method operates. It can be observed the regular way of exploring the space
of variables for SW in contrast to the random procedure for MC. The progressive reduction of the search
interval for IT in both cases is shown. GE also 
performs a random search, but the likelihood of falling at points in the  
vicinity of the optimum increases progressively. However, the DS
method rapidly searches in the direction of the optimum, from the best value
provided by the associated brute force method. 
\FIGVI{sphere-variables-sw-50-50-1.eps}{sphere-variables-mc-2500-1.eps}
{sphere-variables-sw-10-10-25-10-0.5.eps}{sphere-variables-mc-100-25-10-0.1.eps}
{sphere-variables-ge-250-16-0.2-0.2-0.2-32.eps}
{sphere-variables-mc-ra-100-1-600-4-0.1-1.eps}
{Combination of variables simulated by different algorithms}{FigSphereVariables}

Fig.~\ref{FigSphereSWMC} shows the convergence obtained for SW and MC and their
combination with IT. It can
be observed that the convergence improves applying IT. It is important
to note that, although theoretically the convergence increases with IT, 
low values of $N_b$ and $tol$ can cause the optimum value to fall outside of the search
interval ruining the convergence, as is the case of SW+IT-2 and MC+IT-2.
\PLOTII{sphere-evolution-sw.eps}{sphere-evolution-mc.eps}
{Convergence of the optimization for $g_{Sphere}$ with (left) SW and (right) MC,
both combined with IT and different parameters}{FigSphereSWMC}

Fig.~\ref{FigSphereSWMCGR} displays the convergence obtained with SW and MC
coupled with DS. It can be shown that the convergence of DS is vastly superior,
so that in this function the greater the number of simulations with 
the brute force method, the lower the convergence. With regard to the method of
estimating the direction, RA is shown superior to CD.
\PLOTII{sphere-evolution-sw-cdr.eps}{sphere-evolution-mc-cdr.eps}
{Convergence of the optimization for $g_{Sphere}$ with (left) SW and (right) MC,
both combined with DS and different parameters}{FigSphereSWMCGR}

The influence of the different parameters on the convergence for the direction
search method is depicted in Fig.~\ref{FigSphereMCCDR}.
On the top left side, the effect of the parameter $rel$ is illustrated. With a
small step size, null values of $rel$ result in lower values of the convergence.
On the top right side, the influence of the step size is shown. In general,
it is recommended to select a step size of around a few times the expected 
approximated distance between the optimum and the best point obtained by the brute
force method. Finally, in the bottom figure it can be observed that a lower number of direction
estimates ($N_e=2$) results in the fastest convergence with the random value for the
direction search method. 
\FIGIII{sphere-evolution-mc-cd-r.eps}{sphere-evolution-mc-cd-s.eps}
{sphere-evolution-mc-r.eps}
{Convergence of the optimization for $g_{Sphere}$ with Monte-Carlo and direction
search methods and different combinations of parameters}{FigSphereMCCDR}
 
All figures of convergence from~\ref{FigSphereSWMC} to~\ref{FigSphereMCCDR}
have been presented sequentially. This convergence is altered when machines with 
multiple processors are used, due to the distinct parallelization capacity of the 
optimization algorithms. Fig.~\ref{FigSphereSWMCDS} displays the real convergence,
estimated as the maximum number of simulations per task, for $g_{Sphere}$ with SW 
and MC coupled to DS and different combinations of parameters on two machines with 4 
and 64 cores respectively. Given that the computational cost is identical for 
$N_e\leq N_{cores}$, the highest convergence on multiprocessor machines is achieved
for $N_e=\max\PA{N\leq N_{cores}}$, i.e., $N_e=4$ on the 4 core machine and $N_e=10$
on the 64 core machine for the analyzed cases.
\FIGIV{sphere-task-1-4.eps}{sphere-task-1-64.eps}
{sphere-task-2-4.eps}{sphere-task-2-64.eps}
{Convergence of the optimization, estimated as the maximum number of simulations
per task, for $g_{Sphere}$ with SW and MC coupled to DS and different
combinations of parameters on a machine with (left) 4 and (right) 64 cores}
{FigSphereSWMCDS}

Fig.~\ref{FigSphereGE} displays the different convergences for GE with 11
different combinations of parameters. The results, although slightly better than
those of pure SW and MC brute force methods, are clearly inferior to those
obtained with IT and DS. 
\FIGIII{sphere-evolution-ge-1-4.eps}{sphere-evolution-ge-5-8.eps}
{sphere-evolution-ge-9-11.eps}{Convergence of the optimization for $g_{Sphere}$
with GE and different combinations of parameters}{FigSphereGE}

Finally, the results obtained by the 31 different combinations of algorithms
and parameters for the 6 test functions are displayed in
Fig.~\ref{FigTestResults}. 
The result is presented as minus the decimal logarithm of the distance obtained
to the optimum point. This value represents the number of decimal digits of 
precision achieved.
For the methods using random numbers (MC, DS+RA y GE) 10 different
seeds for the pseudo-random number generator have been used. Figure shows, for
these cases, the average value and the extreme values of the error norm obtained
by these seeds. 
The results for the GE can be qualified as disappointing. GE only gets slightly
better results than pure brute force SW and MC methods.
With regard to the brute force methods, the behavior of SW seems to be as a
particular case of MC. Indeed, theoretically, MC takes advantage of situations
with many variables, where the influence of each  variable over the objective
function is different, which is not the case for the analyzed cases.
IT leads to a significant improvement, with a substantial advantage over the
brute force methods in virtually all cases, even though some problems with the
Rosenbrock function. 
Despite the fact that the convergence of DS for the sphere function is higher
using a lower number of simulations for the associated brute force method,
reveals itself as a bad strategy with functions having many local minima. In this case,
a better approach consists of beginning with a sufficient number of simulations
by the stochastic model, in order to obtain a point in the vicinity of the
optimum, and subsequently use the DS for a fast convergence. As regards the
method to estimate the direction, although when RA with a low $N_e$ is used, the
convergence is accelerated, it does not succeed in finding the optimum in some
cases. CD or RA with $N_e=4$ prove to be methods somewhat slower but are deemed
more secure.
\FIGVI{Sphere-e.eps}{Ackley-e.eps}{Booth-e.eps}{Rosenbrock-e.eps}{Easom-e.eps}
{Beale-e.eps}{Precision of the methods, estimated as $-\log_{10}\PA{d_{2500}}$,
in all test cases for a wide set of algorithms and configuration parameters}
{FigTestResults}

All cases presented in this section involve a total of 1705 calibrations and
42625000 simulations. The cases have been run on a laptop
computer equipped with an Intel(R) Core(TM) i7 M620 2.67GHz processor. Total
calculation time was about 3 hours.

\section{Conclusions}

The features and design of MPCOTool, a new open source software tool to enable efficient calibration and optimization of empirical coefficients present in simulation models, has been presented. The program includes the option of two classic brute force models, the sweep and the Monte-Carlo methods, both supplied with the option of an iterative or a direction search procedures to improve convergence. Also, MPCOTool offers the alternative of a genetic algorithm as implemented in Genetic.

The program is easy to use and widely adapted to solve  different optimization/calibration problems offering the user an automatic parallelization to use all available computing resources.

As can be seen in the results of different combinations of functions and methods made in this work, optimization is an art where the selected method and the parameter configuration is strongly conditioned by the objective function properties.

Both the iterative algorithm proposed as the direction search algorithm, coupled to a brute force method, have shown adequate convergence in the objective functions analyzed , dramatically improving the results of pure brute force methods and genetic algorithms.

\section*{Acknowledgements}

This research was funded by the MCINN of the Government of
Spain through grants AGL2010-21681-C03-01, BIA2011-30192-C02-01 and
AGL2013-48728-C2-1-R, and the FPI-MINECO PhD grants program.

\section*{References}
\bibliography{bib}

\end{document}
